---
title: "Learning Kalman Filters, for Tracking Prey"
output:
  html_document:
    df_print: paged
---
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook, used for learning and testing ideas around Kalman Filters. 
I am reading Fundamentals of Kalman Filtering : A Practical Approach 3rd edition by Paul Zarchan and Howard Musoff [3], which offers example code in Fortran. I am reimplementing some examples for learning purposes here and keeping some notes on the way.

### Simulating White Noise
First relevant example I noticed, found on pg 30,is on simulating a simple low-pass filter that is driven under white noise. 
Power spectral density is given by the Fourrier transform of the autocorrelation function. 
One simple and useful form for the power spectral density is that of white noise, in which the power spectral density is constant or $\Phi_xx = \Phi_0$.
The autocorrelation function for white noise is a delta function given by $\phi_xx = \Phi_0 \delta(t)$ (white noise)
Although white noise is not physically realizable, it can serve as an invaluable approximation for situations in which a disturbing noise is wide bandwidth
compared to the system bandwidth.

The example here shows you how to scale the Gaussian noise source so as to approximate the relevant whitenoise *constant powersprectral density* (ex. $\Phi_0=1$), and second that it shows that it applies the 2nd Order Runge-Kutta to integrate the filter's evolution.

Make Low pass filter with pseudowhite noise:
\[y'=(x-y)/T\],
with transfer function in the s-domail : \[\frac{X(s)}{Y(s)} = K(s) =\frac{1}{1+sT} =  =\frac{1}{1+s/\omega_0} \], the filter's cut-off frequency being $\omega_0 = 1/T$.
If x is white noise, then the mean square value of the output of the filter can
also be shown to be : $E[y^2(τ)] = \Phi_0 (1-\exp(-2t/T))/2T$

The standard deviation of the pseudowhite noise (actual white noise over an infinite bandwidth has infinite standard deviation) is related to the desired white noise spectral density $\Phi_0$ and integration interval h according to $\sigma=\sqrt{(\Phi_0/h)}$, where h it the time step of the simulation and Phi_0 has dimensions of units squared per hertz.

The following is an example of a low passfilter output driven by by simulated whitenoise, used to confirm that 68% of responses should be within in sigma of the mean (here zero).

```{r}

tau = 0.2
Phi_0=1
t=0
h = 0.01 ##INtegration TimeStep
sig=sqrt(Phi_0/h) ##scaled Sigma for PseudoRandom White nOise
tLim = 4.999 ##Total Simulation Time
x = vector()
y = vector()
y[1] = 0.0
sigP= vector()
sigM= vector()

i=1
while (t < tLim)
{
  x[i]=rnorm(1,mean=0,sd=sig)
  yd=(x[i]-y[i])/tau
  
  y[i+1] = y[i] + h*yd
  t=t+h ##Increment Time
  ##Doing it 2ce as we are integrating using 2nd Order Runge-Kutta
  yd=(x[i]-y[i+1])/tau
  y[i+1]=(y[i]+y[i+1])/2.0+0.5*h*yd
  sigP[i] = sqrt( Phi_0*(1-exp(-2.0*t/tau) ) / (2*tau) )
  sigM[i] = -sigP[i]
  i=i+1 ##Increment Discrete time Sample index
}

plot(x,type='l',col="black",lty=3) ##Pseudo White Noise Input
lines(y,type='l',col="red",lw=2)
lines(sigP,type='l',lty=3,lw=3)
lines(sigM,type='l',lty=3,lw=3)
legend("topright",lty=c(3,2),col=c("black","red"),legend=c("WT in","Filter out"))
```

The low-pass filter is integrated with timestep h, while the integration is done using 2nd order runge-kutta, instead of simply using euler step derivatives. The 1sigma lines are enveloping the signal. Reducing tau increases the cut-off frequency of the filter, thus letting more noise through.
The integration interval is always chosen to be at least several times
smaller than the smallest time constant $\tau$ $(h << tau)$ in order to get correct answers with numerical integration techniques), the noise will look white to the system. Thus if the difference in  timescale whitenoise to the timestep is large, then it the expected output of the filter should fall within the 1sigma envelope 68%time.

## State Space Notation 

The standard way of representing these systems uses matrix notation :
\[\dot{\bf{x}}  = \bf{Fx} + Gu + w  \], where $u$ is the input vector and $w$ the noise.
The 1st order LP filter from above put in to this form would simply be F=-1/T,G=0,w=\eta/T.

When $F$ is time-invariant then there exists  a *transition * or fundamental matrix able to propagate the state forward from $t_k$ to $t_{k+1}$ as $x(t_{k+1})=\Phi(t_{k+1}-t_{k})x(t_k)$.
One simple way is through the inverse Laplace tranform of : 
\[ \Phi(t) = \mathcal{L}^-1[(sI-F)^-1]   \]
or via Taylor series expansion:
\[ \Phi(t) = \exp^Ft = I +Ft + (Ft)^2/2! + \cdots (Ft)^n/n! + \cdots  \]

Once we have the fundamental matrix we do not have to integrate differential equations to propagate the states forward.
We can propagate states forward by matrix multiplication.
In Kalman filtering we have to propagate state estimates ahead by one sampling time. If we have an exact
expression for the fundamental matrix, we will be able to perform the propagation
by matrix multiplication. When an exact fundamental matrix is not available, we
will have to resort to numerical integration to propagate ahead the state estimates.

## Least Squares Fit - (the gaussian method) 
All of the filtering techniques that will be discussed in this text are,
one way or the other, based on Gauss’s original method of least squares.

1. we will assume a polynomial model to represent the actual signal.
2. we will try to estimate the
coefficients of the selected polynomial by choosing a goodness of fit criterion.
3. we will usecalculus to minimize the sum of the squares of the individual discrepancies in
order to obtain the best coefficients for the selected polynomial.

Let $\hat{x}_k$ be the estimated value and ${x}^*_k$ denote measured values. The sum squared distance measure is to be minimized:
\[R = \sum^n_{k=1}(\hat{x}_k-{x}^*_k)^2. \]
We then use standard calculus techniques and solve for 1st derivative of this series $\dfrac{dR}{d\hat{x}_k} = 0$ to be equal to zero, and the $\dfrac{d^2R}{d\hat{x}_k^2} > 0$.

This method operates as batch processing technique to fit a model to existing data, yet in real time operations estimates are often required as soon as the measurements are being taken.
For polynomial models of degree $n$, with coeffients $a_n$, we would need to solve the system of equations for each derivative $\dfrac{dR}{a_n} = 0$
Thus a matrix inverse has to be evaluated as part of the required computation.
The dimension of the matrix inverse was proportional to the order of the polynomial used to best fit the measurements in the least-squares 
sense.

For a Second-Order or Three-State Least-Squares Filter we have:
\[ \hat{x} = \alpha_0 + \alpha_1 t + \alpha_2 t^2 \]
\[ \hat{\dot{x} } = \alpha_1 + 2 \alpha_2 t\]
\[ \hat{\ddot{x} } = 2 \alpha_2 \]

which can be converted to the discrete-time version by letting $t=(k-1) T_s$.
and finally we minimize the residual sum between model and measurements:
\[R = \sum^n_{k=1}(\hat{x}_k-{x}^*_k)^2 = \sum^n_{k=1} [\alpha_0 + \alpha_1(k-1)T_s + \alpha_2(k-1)^2T^2_s - {x}^*_k]^2 \]

### Recursive Least-squares fitting algorithm 
The batch-processing method of least squares of can be made recursive.
Because the new least-squares filter is recursive, estimates are available as soon as measurements are taken. 

A zero-order (ie constant estimate) filter:
\[\hat{x}_{k+1} = \frac{ \sum_{i=1}^{k+1} x_i^*}{k+1} \]
after manipulating the series by letting \[  \sum_{i=1}^{k} x_i^* = k \hat{x}_k\]
it can then be expressed in the simple recursive form :
\[\hat{x}_{k+1} = \hat{x}_k + \frac{{x}^*_{k+1} - \hat{x}_k}{k+1} \]

If the measurement noise is a zero-mean Gaussian process with variance s 2 n , then a formula can also be derived that describes the
variance of the error in the filter’s estimate (i.e., variance of actual signal minus filter estimate) $x^*_k = x_k+u_k$,
with the variance of the estimate being \[P_k=\sigma_n^2/k \] (see derivation from recursive error), where $\sigma_n^2$ is the variance of the measurement noise.

Define truncation error $\epsilon$ as the difference between the true signal and the estimate:
\[\epsilon_k = x_k - \hat{x}_k \]

we can use this to estimate the theoretical $e_k$ when the signal model and fitted model differ (ex. estimation using zero-order when signal is generated by 1st order filter).
As more measurements are taken k gets larger, and the error in the estimate caused by the measurement noise decreases while the error in the estimate caused by truncation error increases. 
**In principle, for any particular numerical example there is an optimal value of $k$  that will minimize the errors in the estimates caused by both measurement noise and truncation error.**

### 1st order recursive filter

the two gains of a first-order or two-state recursive least-squares filter can be shown(see [1]) to be 
\[K_{1_k} = \frac{ 2(2k-1) }{ k(k+1) } \]
\[ K_{2_k} =  \frac{6}{k(k+1)T_s} \] for $k=1 \cdots n$.

If we think of the measurement as a position, then the estimates from this two-state filter will be position and
velocity (i.e., derivative of position). In other words, position is the first state, whereas the derivative of position or velocity is the second state.
The residual is :
\[ Res_k = x^*_k - \hat{x}_{k-1} - \hat{\dot{x}}_{k-1}T_s \]
The new filter estimates are a combination of the preceding state estimates
projected forward to the current time plus a gain multiplied by the residual or 
\[\hat{x}_{k} = \hat{x}_{k-1} + \hat{\dot{x}}_{k-1}T_s + K_{1_k} Res_k \] 
\[ \hat{\dot{x}}_{k} = \hat{\dot{x}}_{k-1} + K_{2_k} Res_k \]

Formulas can also be derived, using techniques similar to those used on the zeroth-order filter of the preceding section, describing
the variance of the error  in the estimates caused by noise in both states (i.e., $P_{11}$ and $P_{22}$ ) of the first-order filter.

Overall in all up to the three filters the recursive form has the same structure in that the new state estimate is always
the old state estimate projected forward to the current time plus a gain times a residual.

We can see from Figs. 3.34 and 3.35 that, from a measurement noise reduction point of view, it is best to use as low an order filter
as possible because that will tend to reduce the error in the estimate caused by measurement noise.
However, we also know from Table 3.4 that if we make the order of the filter too low there might be excessive truncation error (KL : model bias ).
*In each case the best-order filter to use will depend on the number of measurements to be
taken and the actual order of the signal.*
 

## General filter class : The g-h / $\alpha-\beta$ filters ( [2])

The *key insight* is that we form estimates from the measurement and prediction by forming some kind of blend of the prediction and measurement.
The Kalman and the least-squares filters belong to a general class of filters known as the g-h filter or the α-β filter. 
$g$ and $h$ refer to the two scaling factors that we used in our example.
$g$ is the scaling we used for the measurement  ($x^*$ in our example), and $h$ is the scaling for the change in measurement over time.
$\alpha$ and $\beta$ are  different names used for these same factors.

Each filter has a different way of assigning values to $g$ and $h$, but otherwise the algorithms are identical. For
example, the Benedict-Bordner filter assigns a constant to $g$ and $h$, constrained to a certain range of values.
Other filters such as the Kalman will vary g and h dynamically at each time step.
The general rules are: 

* Multiple data points are more accurate than one data point, so throw nothing away no matter how
inaccurate it is.
* Always choose a number part way between two data points to create a more accurate estimate.
* Predict the next measurement and rate of change based on the current estimate and how much we
think it will change.
* The new estimate is then chosen as part way between the prediction and next measurement scaled by
how accurate each is.


## Polynomial Kalman Filters 

We saw that the recursive least-squares filter provided
the exact same estimates, for a given number of measurements, as the batch-
processing least-squares filter. In this chapter we will first provide the general
equations for the discrete Kalman filter and then show under what conditions it is
completely equivalent to the recursive least-squares filter.
We will also demonstrate the divergence problem with the Kalman filter, but we will show that there are a
variety of engineering fixes.

To apply Kalman-filtering theory, our model of the real world must be described by a set of differential equations. These equations must be cast in
matrix or state-space form as :
\[ \mathbf{ \dot{x}} = \mathbf{Fx + Gu + w} \], where x is a column vector with the states of the system, $F$ the system dynamics
matrix, $u$ is a known vector, which is sometimes called the control vector, and $w$
is a white-noise process, which is also expressed as a vector.
There is a process-noise matrix $\mathcal{Q}$ that is related to the process-noise vector according to : 
\[\mathbf{Q} = \mathbb{E}[\mathbf{ww^T}] \],
which is sometimes used as a device for telling the filter that we know the filter’s model of the real world is not precise.
The Kalman-filter formulation requires that the measurements be linearly related to the states according to :
\[  \mathbf{z} = \mathbf{Hx +v}  \]
where $ \mathbf{z} $ is the measurement vector, $ \mathbf{H}$ is the measurement matrix that relates the state to the measurement, and $ \mathbf{v}$ is white measurement noise, which is also expressed as a vector.


The discrete Kalman-filtering equation is given by : 
\[ \mathbf{ \hat{x}_k } = \Phi_k  \hat{x}_{k-1} + G_k u_{k-1} + K_k(z_k - H \Phi_k \hat{x}_{k-1} - H G_k u_{k-1})\]
where $\Phi_k$ is the discrete version of the fundamental matrix (see sections above) evaluated as $\Phi_k = \Phi(T_s) $,
where $K_k$ represents the Kalman gain matrix and $G_k$ is obtained from 
\[G_k = \int_0^{T_s} \Phi (\tau)G d\tau \]
if $u_{k-1}$ is assumed to be constant between sampling instants.
The Kalman gains are computed, while the filter is operating, from the matrix Riccati equations.
The Riccati equations are a set of recursive matrix equations given by :
\[ M_k = \Phi_k P_{k-1} \Phi^T_k + Q_k \]
\[K_k = M_k H^T (HM_k H^T + R_k)^{-1}\]
\[ P_k = I - K_k H)M_k \]
where $P_k$ is a covariance matrix representing errors in the state estimates (i.e., variance of truth minus estimate) after an update and M k is the covariance matrix representing errors in the state estimates before an update.
The discrete process-noise matrix $Q$ k can be found from the continuous process-noise matrix $Q$ and the
fundamental matrix according to
\[ Q_k = \int^{T_s}_0 \Phi(\tau)Q \Phi(\tau) d\tau\] ( KL:Corrected for  $d\tau$ )
To start the Riccati equations, we need an initial covariance matrix $P_0$.

The gain of the Kalman filter *is chosen to minimize the variance of the error in the estimate.*
The Riccati equations are simply an iterative way of finding the optimal gain at each time
step.



## References 

1. Kalman and Bayesian Filters in Python, Roger R Labbe Jr
2. Morrison, N., Introduction to Sequential Smoothing and Prediction, McGraw–Hill,
New York, 1969, pp. 339–376.
3. Fundamentals of Kalman Filtering: A practical approach, 3rd ed., Paul Zarchan, Howard Musoff