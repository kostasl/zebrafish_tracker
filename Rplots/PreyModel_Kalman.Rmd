---
title: "Learning Kalman Filters, for Tracking Prey"
output:
  html_document:
    df_print: paged
---
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook, used for learning and testing ideas around Kalman Filters. 
I am reading Fundamentals of Kalman Filtering : A Practical Approach 3rd edition by Paul Zarchan and Howard Musoff, which offers example code in Fortran. I am reimplementing some examples for learning purposes here and keeping some notes on the way.

### Simulating White Noise
First relevant example I noticed, found on pg 30,is on simulating a simple low-pass filter that is driven under white noise. 
Power spectral density is given by the Fourrier transform of the autocorrelation function. 
One simple and useful form for the power spectral density is that of white noise, in which the power spectral density is constant or $\Phi_xx = \Phi_0$.
The autocorrelation function for white noise is a delta function given by $\phi_xx = \Phi_0 \delta(t)$ (white noise)
Although white noise is not physically realizable, it can serve as an invaluable approximation for situations in which a disturbing noise is wide bandwidth
compared to the system bandwidth.

The example here shows you how to scale the Gaussian noise source so as to approximate the relevant whitenoise *constant powersprectral density* (ex. $\Phi_0=1$), and second that it shows that it applies the 2nd Order Runge-Kutta to integrate the filter's evolution.

Make Low pass filter with pseudowhite noise:
\[y'=(x-y)/T\],
with transfer function in the s-domail : \[\frac{X(s)}{Y(s)} = K(s) =\frac{1}{1+sT} =  =\frac{1}{1+s/\omega_0} \], the filter's cut-off frequency being $\omega_0 = 1/T$.
If x is white noise, then the mean square value of the output of the filter can
also be shown to be : $E[y^2(τ)] = \Phi_0 (1-\exp(-2t/T))/2T$

The standard deviation of the pseudowhite noise (actual white noise over an infinite bandwidth has infinite standard deviation) is related to the desired white noise spectral density $\Phi_0$ and integration interval h according to $\sigma=\sqrt{(\Phi_0/h)}$, where h it the time step of the simulation and Phi_0 has dimensions of units squared per hertz.

The following is an example of a low passfilter output driven by by simulated whitenoise, used to confirm that 68% of responses should be within in sigma of the mean (here zero).

```{r}

tau = 0.2
Phi_0=1
t=0
h = 0.01 ##INtegration TimeStep
sig=sqrt(Phi_0/h) ##scaled Sigma for PseudoRandom White nOise
tLim = 4.999 ##Total Simulation Time
x = vector()
y = vector()
y[1] = 0.0
sigP= vector()
sigM= vector()

i=1
while (t < tLim)
{
  x[i]=rnorm(1,mean=0,sd=sig)
  yd=(x[i]-y[i])/tau
  
  y[i+1] = y[i] + h*yd
  t=t+h ##Increment Time
  ##Doing it 2ce as we are integrating using 2nd Order Runge-Kutta
  yd=(x[i]-y[i+1])/tau
  y[i+1]=(y[i]+y[i+1])/2.0+0.5*h*yd
  sigP[i] = sqrt( Phi_0*(1-exp(-2.0*t/tau) ) / (2*tau) )
  sigM[i] = -sigP[i]
  i=i+1 ##Increment Discrete time Sample index
}

plot(x,type='l',col="black",lty=3) ##Pseudo White Noise Input
lines(y,type='l',col="red",lw=2)
lines(sigP,type='l',lty=3,lw=3)
lines(sigM,type='l',lty=3,lw=3)
legend("topright",lty=c(3,2),col=c("black","red"),legend=c("WT in","Filter out"))
```

The low-pass filter is integrated with timestep h, while the integration is done using 2nd order runge-kutta, instead of simply using euler step derivatives. The 1sigma lines are enveloping the signal. Reducing tau increases the cut-off frequency of the filter, thus letting more noise through.
The integration interval is always chosen to be at least several times
smaller than the smallest time constant $\tau$ $(h << tau)$ in order to get correct answers with numerical integration techniques), the noise will look white to the system. Thus if the difference in  timescale whitenoise to the timestep is large, then it the expected output of the filter should fall within the 1sigma envelope 68%time.

## State Space Notation 

The standard way of representing these systems uses matrix notation :
\[\dot{\bf{x}}  = \bf{Fx} + Gu + w  \], where $u$ is the input vector and $w$ the noise.
The 1st order LP filter from above put in to this form would simply be F=-1/T,G=0,w=\eta/T.

When $F$ is time-invariant then there exists  a *transition * or fundamental matrix able to propagate the state forward from $t_k$ to $t_{k+1}$ as $x(t_{k+1})=\Phi(t_{k+1}-t_{k})x(t_k)$.
One simple way is through the inverse Laplace tranform of : 
\[ \Phi(t) = \mathcal{L}^-1[(sI-F)^-1]   \]
or via Taylor series expansion:
\[ \Phi(t) = \exp^Ft = I +Ft + (Ft)^2/2! + \cdots (Ft)^n/n! + \cdots  \]

Once we have the fundamental matrix we do not have to integrate differential equations to propagate the states forward.
We can propagate states forward by matrix multiplication.
In Kalman filtering we have to propagate state estimates ahead by one sampling time. If we have an exact
expression for the fundamental matrix, we will be able to perform the propagation
by matrix multiplication. When an exact fundamental matrix is not available, we
will have to resort to numerical integration to propagate ahead the state estimates.

## Least Squares Fit - (the gaussian method) 
All of the filtering techniques that will be discussed in this text are,
one way or the other, based on Gauss’s original method of least squares.

1. we will assume a polynomial model to represent the actual signal.
2. we will try to estimate the
coefficients of the selected polynomial by choosing a goodness of fit criterion.
3. we will usecalculus to minimize the sum of the squares of the individual discrepancies in
order to obtain the best coefficients for the selected polynomial.

Let $\hat{x}_k$ be the estimated value and ${x}^*_k$ denote measured values. The sum squared distance measure is to be minimized:
\[R = \sum^n_{k=1}(\hat{x}_k-{x}^*_k)^2. \]
We then use standard calculus techniques and solve for 1st derivative of this series $\dfrac{dR}{d\hat{x}_k} = 0$ to be equal to zero, and the $\dfrac{d^2R}{d\hat{x}_k^2} > 0$.

This method operates as batch processing technique to fit a model to existing data, yet in real time operations estimates are often required as soon as the measurements are being taken.
For polynomial models of degree $n$, with coeffients $a_n$, we would need to solve the system of equations for each derivative $\dfrac{dR}{a_n} = 0$
Thus a matrix inverse has to be evaluated as part of the required computation.
The dimension of the matrix inverse was proportional to the order of the polynomial used to best fit the measurements in the least-squares 
sense.

For a Second-Order or Three-State Least-Squares Filter we have:
\[ \hat{x} = \alpha_0 + \alpha_1 t + \alpha_2 t^2 \]
\[ \hat{\dot{x} } = \alpha_1 + 2 \alpha_2 t\]
\[ \hat{\ddot{x} } = 2 \alpha_2 \]

which can be converted to the discrete-time version by letting $t=(k-1) T_s$.
and finally we minimize the residual sum between model and measurements:
\[R = \sum^n_{k=1}(\hat{x}_k-{x}^*_k)^2 = \sum^n_{k=1} [\alpha_0 + \alpha_1(k-1)T_s + \alpha_2(k-1)^2T^2_s - {x}^*_k]^2 \]

### Recursive Least-squares fitting algorithm 
The batch-processing method of least squares of can be made recursive.
Because the new least-squares filter is recursive, estimates are available as soon as measurements are taken. 

A zero-order (ie constant estimate) filter:
\[\hat{x}_{k+1} = \frac{ \sum_{i=1}^{k+1} x_i^*}{k+1} \]
after manipulating the series by letting \[  \sum_{i=1}^{k} x_i^* = k \hat{x}_k\]
it can then be expressed in the simple recursive form :
\[\hat{x}_{k+1} = \hat{x}_k + \frac{{x}^*_{k+1} - \hat{x}_k}{k+1} \]

If the measurement noise is a zero-mean Gaussian process with variance s 2 n , then a formula can also be derived that describes the
variance of the error in the filter’s estimate (i.e., variance of actual signal minus filter estimate) $x^*_k = x_k+u_k$,
with the variance of the estimate being \[P_k=\sigma_n^2/k \] (see derivation from recursive error), where $\sigma_n^2$ is the variance of the measurement noise.

Define truncation error $\epsilon$ as the difference between the true signal and the estimate:
\[\epsilon_k = x_k - \hat{x}_k \]

we can use this to estimate the theoretical $e_k$ when the signal model and fitted model differ (ex. estimation using zero-order when signal is generated by 1st order filter).
As more measurements are taken k gets larger, and the error in the estimate caused by the measurement noise decreases while the error in the estimate caused by truncation error increases. 
**In principle, for any particular numerical example there is an optimal value of $k$  that will minimize the errors in the estimates caused by both measurement noise and truncation error.**

### 1st order recursive filter

the two gains of a first-order or two-state recursive least-squares filter can be shown to be 
\[K_{1_k} = \frac{ 2(2k-1) }{ k(k+1) } \]
\[ K_{2_k} =  \frac{6}{k(k+1)T_s} \] for $k=1 \cdots n$.

If we think of the measurement as a position, then the estimates from this two-state filter will be position and
velocity (i.e., derivative of position). In other words, position is the first state, whereas the derivative of position or velocity is the second state.
The residual is :
\[ Res_k = x^*_k - \hat{x}_{k-1} - \hat{\dot{x}}_{k-1}T_s \]
The new filter estimates are a combination of the preceding state estimates
projected forward to the current time plus a gain multiplied by the residual or 
\[\hat{x}_{k} = \hat{x}_{k-1} + \hat{\dot{x}}_{k-1}T_s + K_{1_k} Res_k \] 
\[ \hat{\dot{x}}_{k} = \hat{\dot{x}}_{k-1} + K_{2_k} Res_k \]

Formulas can also be derived, using techniques similar to those used on the zeroth-order filter of the preceding section, describing
the variance of the error  in the estimates caused by noise in both states (i.e., $P_{11}$ and $P_{22}$ ) of the first-order filter.